{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monotonized Fully-Connected network with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torch_data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the dataset.\n",
    "digits, targets = load_digits(return_X_y=True)\n",
    "digits = digits.astype(np.float32) / 255\n",
    "\n",
    "digits_train, digits_test, targets_train, targets_test = train_test_split(digits, targets, random_state=0)\n",
    "\n",
    "train_size = digits_train.shape[0]\n",
    "\n",
    "input_size = 8*8\n",
    "classes_n = 10\n",
    "EPOCHS = 200\n",
    "\n",
    "train_batch_size = 30 \n",
    "val_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTData(torch_data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(MNISTData, self).__init__()\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx],self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = MNISTData(digits_train, targets_train) \n",
    "val_dset = MNISTData(digits_test, targets_test) \n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=train_batch_size, shuffle=True) \n",
    "val_loader = torch_data.DataLoader(val_dset, batch_size=val_batch_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FCN, self).__init__()\n",
    "        self.fc0 = nn.Linear(64, 40)\n",
    "        self.fc1 = nn.Linear(40, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        h1 = F.relu(self.fc0(input_))\n",
    "        h2 = F.relu(self.fc1(h1))\n",
    "        h3 = self.fc2(h2)\n",
    "        return h3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_sparcity(model, layers=[0,1], verbose=0):\n",
    "    zero_total = 0\n",
    "    n_total = 0\n",
    "    for layer_name, layer in model.named_modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer,nn.Linear):\n",
    "            zeros = (layer.weight.data == 0).sum().cpu().numpy()\n",
    "            n = np.prod(layer.weight.data.shape)\n",
    "            zero_total += zeros\n",
    "            n_total += n\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f'Layer: {l}, 0-weights ratio: {zeros/n:.4}')\n",
    "\n",
    "    return zero_total/n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(net, dataloader):\n",
    "    test_loader = torch_data.DataLoader(val_dset,batch_size = len(val_dset)) \n",
    "    net.eval()\n",
    "    for X,y in dataloader:\n",
    "        #X = X.to(device)\n",
    "        nn_outputs = net(X).detach().numpy().argmax(axis = 1)\n",
    "        return accuracy_score(nn_outputs,y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monotonize(model, pos=True, ignore_last=True):\n",
    "    for n, x in model.named_parameters():\n",
    "        if 'weight' in n:\n",
    "            if ignore_last and n == 'fc2.weight':\n",
    "                continue\n",
    "            attrs = n.split('.')\n",
    "            obj = model\n",
    "            for attr in attrs[:-1]:\n",
    "                obj = getattr(obj, attr)\n",
    "\n",
    "        if pos:\n",
    "            obj.weight.data = obj.weight.clip(min=0)\n",
    "        else:\n",
    "            obj.weight.data = obj.weight.clip(max=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, net, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True,\n",
    "          save_dir=None, monotonize = None, L1_lambda = 0.01):\n",
    "    if monotonize is not None:\n",
    "        monotonize(net)\n",
    "        if L1_lambda:\n",
    "            print('Monotonized with l1')\n",
    "        else:\n",
    "            print('Monotonized')\n",
    "    else:\n",
    "        if not L1_lambda:\n",
    "            print(\"Not monotonized\")\n",
    "    \n",
    "    net.to(device)\n",
    "    total_loss, accuracy_train, accuracy_test, sparsity = [], [], [], []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        loss = []\n",
    "        for X, y in train_loader:\n",
    "            nn_outputs = net(X)\n",
    "            loss1 = criterion(nn_outputs, y)\n",
    "            optimizer.zero_grad()\n",
    "            if L1_lambda:\n",
    "                for name, param in net.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        L1 = Variable(param, requires_grad=True)\n",
    "                        L1 = torch.norm(L1, 1)\n",
    "                        L1 = L1_lambda * L1\n",
    "                        loss1 = loss1 + L1\n",
    "            loss1.backward()\n",
    "            loss.append(loss1.item())\n",
    "            optimizer.step()\n",
    "            if monotonize is not None:\n",
    "                monotonize(net)\n",
    "            \n",
    "\n",
    "        net.eval()\n",
    "        \n",
    "        val_loss = []\n",
    "        for X, y in val_loader:\n",
    "            nn_outputs = net(X)\n",
    "            val_loss1 = criterion(nn_outputs,y)\n",
    "            val_loss.append(val_loss1.item())\n",
    "        \n",
    "        total_loss.append(np.mean(loss))\n",
    "        \n",
    "        acc_tr = get_accuracy(net, train_loader)\n",
    "        acc_va = get_accuracy(net, val_loader)\n",
    "        \n",
    "        accuracy_train.append(acc_tr)\n",
    "        accuracy_test.append(acc_va)\n",
    "        \n",
    "        spars = report_sparcity(net)\n",
    "        sparsity.append(spars)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        freq = max(epochs//20,1)\n",
    "        if verbose and epoch%freq==0:\n",
    "            print('Epoch {}/{} \\t || Loss:  Train {:.3f} | Val {:.3f} || Accuracy: Train {:.3f} | Val {:.3f}\\\n",
    " || Sparsity: {:.3f} '.format(epoch, epochs, np.mean(loss), np.mean(val_loss),acc_tr ,acc_va, spars ))\n",
    "\n",
    "            \n",
    "    return total_loss, accuracy_train, accuracy_test, sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(loss, train, val, sp):\n",
    "    _, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
    "\n",
    "    ax[0].plot(range(1, len(loss)+1), loss)\n",
    "    ax[0].set_title('Loss', fontsize=14)\n",
    "\n",
    "    ax[1].plot(range(1, len(train)+1), train, label='train')\n",
    "    ax[1].plot(range(1, len(train)+1), val, label='validation')\n",
    "    ax[1].set_title('Accuracy', fontsize=14)\n",
    "    ax[1].set_xlabel('epoch', fontsize=14)\n",
    "    ax[1].legend();\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(sp)\n",
    "    plt.title('Model sparsity (0-weights ratio)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not monotonized\n",
      "Epoch 10/200 \t || Loss:  Train 1.253 | Val 1.234 || Accuracy: Train 0.800 | Val 0.750 || Sparsity: 0.000 \n",
      "Epoch 20/200 \t || Loss:  Train 0.570 | Val 0.591 || Accuracy: Train 0.767 | Val 0.880 || Sparsity: 0.000 \n",
      "Epoch 30/200 \t || Loss:  Train 0.410 | Val 0.430 || Accuracy: Train 0.833 | Val 0.900 || Sparsity: 0.000 \n",
      "Epoch 40/200 \t || Loss:  Train 0.326 | Val 0.350 || Accuracy: Train 0.867 | Val 0.920 || Sparsity: 0.000 \n",
      "Epoch 50/200 \t || Loss:  Train 0.271 | Val 0.305 || Accuracy: Train 0.900 | Val 0.950 || Sparsity: 0.000 \n",
      "Epoch 60/200 \t || Loss:  Train 0.234 | Val 0.269 || Accuracy: Train 0.933 | Val 0.970 || Sparsity: 0.000 \n",
      "Epoch 70/200 \t || Loss:  Train 0.207 | Val 0.248 || Accuracy: Train 0.933 | Val 0.960 || Sparsity: 0.000 \n",
      "Epoch 80/200 \t || Loss:  Train 0.185 | Val 0.242 || Accuracy: Train 0.900 | Val 0.960 || Sparsity: 0.000 \n",
      "Epoch 90/200 \t || Loss:  Train 0.163 | Val 0.223 || Accuracy: Train 0.967 | Val 0.960 || Sparsity: 0.000 \n",
      "Epoch 100/200 \t || Loss:  Train 0.146 | Val 0.211 || Accuracy: Train 1.000 | Val 0.970 || Sparsity: 0.000 \n",
      "Epoch 110/200 \t || Loss:  Train 0.131 | Val 0.211 || Accuracy: Train 1.000 | Val 0.960 || Sparsity: 0.000 \n",
      "Epoch 120/200 \t || Loss:  Train 0.117 | Val 0.202 || Accuracy: Train 0.933 | Val 0.950 || Sparsity: 0.000 \n",
      "Epoch 130/200 \t || Loss:  Train 0.108 | Val 0.194 || Accuracy: Train 1.000 | Val 0.950 || Sparsity: 0.000 \n",
      "Epoch 140/200 \t || Loss:  Train 0.097 | Val 0.194 || Accuracy: Train 1.000 | Val 0.950 || Sparsity: 0.000 \n",
      "Epoch 150/200 \t || Loss:  Train 0.089 | Val 0.187 || Accuracy: Train 1.000 | Val 0.950 || Sparsity: 0.000 \n",
      "Epoch 160/200 \t || Loss:  Train 0.081 | Val 0.191 || Accuracy: Train 0.967 | Val 0.950 || Sparsity: 0.000 \n",
      "Epoch 170/200 \t || Loss:  Train 0.074 | Val 0.184 || Accuracy: Train 1.000 | Val 0.950 || Sparsity: 0.000 \n",
      "Epoch 180/200 \t || Loss:  Train 0.067 | Val 0.195 || Accuracy: Train 1.000 | Val 0.950 || Sparsity: 0.000 \n",
      "Epoch 190/200 \t || Loss:  Train 0.061 | Val 0.189 || Accuracy: Train 1.000 | Val 0.960 || Sparsity: 0.000 \n",
      "Epoch 200/200 \t || Loss:  Train 0.055 | Val 0.203 || Accuracy: Train 1.000 | Val 0.960 || Sparsity: 0.000 \n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = FCN() \n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "scheduler = None\n",
    "net_res = train(EPOCHS, net, criterion, optimizer, train_loader, val_loader, scheduler, monotonize=None, L1_lambda=0.000)\n",
    "#%time show_results(*train(EPOCHS, net, criterion, optimizer, train_loader, val_loader, scheduler, monotonize=None, L1_lambda=0.000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monotonized\n",
      "Epoch 10/200 \t || Loss:  Train 1.755 | Val 1.729 || Accuracy: Train 0.533 | Val 0.410 || Sparsity: 0.419 \n",
      "Epoch 20/200 \t || Loss:  Train 1.122 | Val 1.143 || Accuracy: Train 0.800 | Val 0.660 || Sparsity: 0.391 \n",
      "Epoch 30/200 \t || Loss:  Train 0.870 | Val 0.917 || Accuracy: Train 0.767 | Val 0.690 || Sparsity: 0.349 \n",
      "Epoch 40/200 \t || Loss:  Train 0.717 | Val 0.756 || Accuracy: Train 0.800 | Val 0.760 || Sparsity: 0.325 \n",
      "Epoch 50/200 \t || Loss:  Train 0.630 | Val 0.664 || Accuracy: Train 0.767 | Val 0.800 || Sparsity: 0.322 \n",
      "Epoch 60/200 \t || Loss:  Train 0.563 | Val 0.607 || Accuracy: Train 0.833 | Val 0.800 || Sparsity: 0.320 \n",
      "Epoch 70/200 \t || Loss:  Train 0.520 | Val 0.560 || Accuracy: Train 0.867 | Val 0.840 || Sparsity: 0.259 \n",
      "Epoch 80/200 \t || Loss:  Train 0.476 | Val 0.529 || Accuracy: Train 0.833 | Val 0.850 || Sparsity: 0.277 \n",
      "Epoch 90/200 \t || Loss:  Train 0.439 | Val 0.484 || Accuracy: Train 0.967 | Val 0.870 || Sparsity: 0.263 \n",
      "Epoch 100/200 \t || Loss:  Train 0.395 | Val 0.445 || Accuracy: Train 0.933 | Val 0.860 || Sparsity: 0.289 \n",
      "Epoch 110/200 \t || Loss:  Train 0.356 | Val 0.412 || Accuracy: Train 0.867 | Val 0.870 || Sparsity: 0.294 \n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net_mon = FCN() \n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.Adam(net_mon.parameters())\n",
    "scheduler = None\n",
    "net_mon_res = train(EPOCHS, net_mon, criterion, optimizer, train_loader, val_loader, scheduler, monotonize=monotonize, L1_lambda=0.000)\n",
    "#%time show_results(*train(EPOCHS, net_mon, criterion, optimizer, train_loader, val_loader, scheduler, monotonize=monotonize, L1_lambda=0.000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net_mon_l1 = FCN() \n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.Adam(net_mon_l1.parameters())\n",
    "scheduler = None\n",
    "net_mon_l1_res = train(EPOCHS, net_mon_l1, criterion, optimizer, train_loader, val_loader, scheduler, monotonize=monotonize, L1_lambda=0.0001)\n",
    "#%time show_results(*train(EPOCHS, net_mon_l1, criterion, optimizer, train_loader, val_loader, scheduler, monotonize=monotonize, L1_lambda=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, train, val, sp = net_res\n",
    "loss1, train1, val1, sp1 = net_mon_res\n",
    "loss2, train2, val2, sp2 = net_mon_l1_res\n",
    "_, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
    "\n",
    "ax[0].plot(range(1, len(loss)+1), loss, label='not monotonized', lw=3)\n",
    "ax[0].plot(range(1, len(loss1)+1), loss1, label='monotonized', lw=3)\n",
    "ax[0].plot(range(1, len(loss2)+1), loss2, label='monotonized with l1', lw=3)\n",
    "ax[0].set_title('Loss', fontsize=14)\n",
    "ax[0].set_xlabel('Epoch', fontsize=14)\n",
    "ax[0].legend(fontsize=12)\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(range(1, len(train)+1), val, label='not monotonized', lw=3)\n",
    "ax[1].plot(range(1, len(train)+1), val1, label='monotonized', lw=3)\n",
    "ax[1].plot(range(1, len(train)+1), val2, label='monotonized with l1', lw=3)\n",
    "\n",
    "ax[1].set_title('Accuracy', fontsize=14)\n",
    "ax[1].set_xlabel('Epoch', fontsize=14)\n",
    "ax[1].legend(fontsize=12, loc='lower right');\n",
    "ax[1].grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(sp)+1), sp, label='not monotonized', lw=3)\n",
    "plt.plot(range(1, len(sp1)+1), sp1, label='monotonized', lw=3)\n",
    "plt.plot(range(1, len(sp2)+1), sp2, label='monotonized with l1', lw=3)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid()\n",
    "plt.title('Model sparsity (0-weights ratio)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = [net, net_mon, net_mon_l1]\n",
    "\n",
    "def generate_monotone_sequence(example):\n",
    "    n_features = example.shape[1]\n",
    "    seq = np.arange(100 * n_features).reshape(100, n_features)\n",
    "\n",
    "    return torch.tensor(seq, dtype=example.dtype)\n",
    "\n",
    "def check_monotonicity(self, input, output):\n",
    "    for i in range(input[0].shape[1]):\n",
    "        res = input[0][:, i].detach().numpy()\n",
    "        print(f'Neuron #{i} \\t (the last layer) has monotone input: {np.all(np.diff(res) >= 0)}')\n",
    "        \n",
    "seq = generate_monotone_sequence(train_dset.X) / 6400.\n",
    "\n",
    "handle = nets[1].fc2.register_forward_hook(check_monotonicity)\n",
    "\n",
    "_ = nets[1](seq)\n",
    "\n",
    "handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2 = generate_monotone_sequence(train_dset.X) / 6400.\n",
    "\n",
    "handle = nets[2].fc2.register_forward_hook(check_monotonicity)\n",
    "\n",
    "_ = nets[2](seq)\n",
    "\n",
    "handle.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
